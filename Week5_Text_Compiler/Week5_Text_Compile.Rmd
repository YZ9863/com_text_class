---
title: "Week5 - Text Compiler & Bigrams"
author: "Yiran Zhang"
date: "2025-02-27"
output: html_document
---

#Jour 689: This code, largely written by the famous **Sean Mussenden**, takes separate text files and compiles them into a single dataframe for analysis.

1)  Create a folder Week5_Text_Compiler
2)  Copy your spreadsheet index and the folder of text files into Week5_Text_Compiler
3)  Create an .Rproj file for Week5_Text_Compiler

```{r}
library(tidyverse)
library(janitor)
#install.packages("striprtf")
library(striprtf)
```

# Reformat .RTF files

```{r}
# Load required packages

# Set the paths for your folders
input_folder <- "/Users/yiranzhang/Desktop/4B movement_Articles/Full Articles"  # Replace with your input folder path
output_folder <- "./4Bfile/" # Replace with your output folder path

# Create output folder if it doesn't exist
if (!dir.exists(output_folder)) {
  dir.create(output_folder)
}

# Get a list of all .rtf files in the input folder
rtf_files <- list.files(path = input_folder, pattern = "\\.RTF$", full.names = TRUE)

# Convert each .rtf file to .txt
for (file in rtf_files) {
  # Extract the file name without extension
  file_name <- tools::file_path_sans_ext(basename(file))
  
  # Read the RTF content
  rtf_content <- read_rtf(file)
  
  # Create output file path
  output_file <- file.path(output_folder, paste0(file_name, ".txt"))
  
  # Write the content to a .txt file
  writeLines(rtf_content, output_file)
  
  # Print progress
  cat("Converted:", file, "to", output_file, "\n")
}

cat("Conversion complete!\n")
```

# Raw text compiler

```{r include=FALSE}
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################

###
# List out text files that match pattern .txt, create DF
###

# Adjust thisline for your file name
files <- list.files("./4Bfile", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an matching file name
  mutate(index = str_replace_all(filename, ".txt", "")) %>%
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", index))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)

#Join the file list to the index

final_data <- rio::import("Results list for__4B movement_.XLSX") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)
```

### Check for duplicate entries

```{r}
final_data |> 
  count(title) |> 
  arrange(desc(n))
```

# why did it drop from 90 to 68?

```{r}
dupe_data <- rio::import("Results list for__4B movement_.XLSX") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25))
```
```{r}
dupe_data |> 
  count(title) |> 
  arrange(desc(n))
```




```{r}

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  mutate(filepath = paste0("./4Bfile/", filename))

head(final_index)
```

#Fact Check

```{r}

anti_final_index <- final_data |> 
  anti_join(files, c("index"))

```

#Checking for duplicates

```{r}
final_index |> 
  count(title) |> 
  arrange(desc(n))

```

#Text compiler

```{r}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)


write.csv(articles_df, "/Users/yiranzhang/Desktop/code/com_text_class/Week5_Text_Compiler/4B_FullArticles.csv")

```


# Bigrams Part Starts Here #


```{r}
#load tidyverse, tidytext, rio, janitor libraries

library(tidyverse)
library(tidytext)
library(janitor)
library(rio)
library(dplyr)

```


# I first rename the file as project.

```{r}
project <- articles_df
rm(articles_df)

project <- read_csv("4B_FullArticles.csv")
```

# Tokenize the sentence column
Copy the code from the in-class bigrams exercise and tokenize just the sentence column from your dataframe

Hint: you're changing just one variable

```{r}
sentence <- str_replace_all(project$sentence, "- ", "")
sentence_tk <- tibble(sentence,)

sentence_tokenized <- sentence_tk %>%
  unnest_tokens(word,sentence)

sentence_tokenized
```

#Remove stopwords and count the words
```{r}
stopwords <- stop_words
stopwords

stopwords |> 
  count(lexicon)

```


```{r}

data(stop_words)

sentence_tokenized <- sentence_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

sentence_word_ct <- sentence_tokenized %>%
  count(word, sort=TRUE)

sentence_word_ct

write_csv(sentence_word_ct, "sentence_word_ct.csv")
```

# Create Bigrams

```{r}
bigrams <- sentence_tk %>%
  unnest_tokens(bigram, sentence, token="ngrams", n=2)

bigrams

#Filter out stop words.

bigrams1 <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams2 <- bigrams1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) |>
  filter(!is.na(word1)) |>
  filter(!is.na(word2)) 

bigram3 <- bigrams2 %>%
  count(word1, word2, sort = TRUE)

bigram3

```


```{r}
# 📌 加载必要的库
library(igraph)
library(ggraph)
library(tidyverse)


# 1️⃣ 只保留出现频率较高的 bigram（避免图太乱）
bigram_filtered <- bigram3 %>%
  filter(n > 45)  # 可以调整阈值

# 2️⃣ 构建网络图数据
bigram_graph <- graph_from_data_frame(bigram_filtered)

# 3️⃣ 画 Network Graph
set.seed(123)  # 保持布局一致
ggraph(bigram_graph, layout = "fr") +  # "fr" = Fruchterman-Reingold布局（常用）
  geom_edge_link(aes(edge_alpha = n, edge_width = n), color = "gray") +  
  geom_node_point(size = 5, color = "skyblue") +  
  geom_node_text(aes(label = name), repel = TRUE) +  
  labs(title = "Bigram Network Graph",
       subtitle = "Frequently Occurring Word Pairs",
       x = NULL, y = NULL) +  
  theme_void()  # 去除背景网格
```


# Write a 250-300 word memo describing your findings in the bigrams. Note any words that should be cleaned out. Also note any issues or problems you see.

Based on these high-frequency bigrams, I found several key themes in this dataset. First, the phrase “South Korea” suggests that the 4B movement originally came from South Korea. It is not a product of American culture. Second, phrases related to politices  (such as “Donald Trump”, “kamala harris”, “joe biden”, “white house”, “president elect”, etc.) show that the emergence of the 4B movement in the US is a response to the president election. In other words, Trump’s presidency was a catalyst that triggered the presence of the 4B movement in America.

The third group of words is  related to women’s rights, including “women’s rights”, “feminism women’s”, “gender equality”, “reproductive rights”, “abortion rights”, “gender sex”, and “sex discrimination”. These words indicate that, unlike the original 4B movement in Korea (which stresses gender separatism by rejecting dating, sex, marriage, or having children with men), the American 4B movement focuses more on promoting gender equality, opposing sexual discrimination, and emphasizing reproductive and abortion rights. It also suggests that it is Trump’s perceived threat to these rights that caused the emergence of the 4B movement in the US.

The fourth group of words is related to media, such as “social media”, “internet social”, and “social networking”. These words suggest that, unlike traditional social movements, the 4B movement is primarily an online movement. This suggests that we should explore discussions on social media to fully understand this movement.

I have two questions here. First, there is also a group of bigrams associated with news or copyright (such as “rights reserved,” “copyright 2024,” “language english”), which I believe is related to metadata rather than the text itself. Thus, I wonder how to deal with those bigrams. In addition, although those bigrams of this dataset are objective and appealing, the way we group and interpret them seems subjective to me. I want to know how to deal with this subjectivity.